{
  "metadata": {
    "name": "Analysis and store",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndate \u003d \"2023/09/20\"\ndf \u003d spark.read.parquet(\"s3a://cis-refined/scores/\" + date)\n\nresult \u003d df.select(\"benchmark-id\", \"benchmark-version\").distinct().collect()\nfor r in result:\n    print(r)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndate \u003d \"2023/09/20\"\ndf_scores \u003d spark.read.parquet(\"s3a://cis-refined/scores/\" + date)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\ndf_scores.withColumn(\"date\", F.to_date(\"date\"))\nwindow_spec \u003d Window.partitionBy(\"server\").orderBy(F.col(\"date\").desc())\ndf_with_rank \u003d df_scores.withColumn(\"rank\", F.row_number().over(window_spec))\nresult \u003d df_with_rank.filter(F.col(\"rank\") \u003d\u003d 1).drop(\"rank\")\nresult.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\njdbc_url \u003d \"jdbc:postgresql://postgresql:5432/postgres\"\njdbc_properties \u003d {\n    \"user\": \"postgres\",\n    \"password\": \"abc123!\",\n    \"driver\": \"org.postgresql.Driver\"\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nresult.write.jdbc(jdbc_url, \"last_scores\", mode\u003d\"overwrite\", properties\u003djdbc_properties)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_last_month \u003d df_results\ndf_last_month.withColumn(\"date\", F.to_date(\"date\"))\nfiltered_df \u003d df_results.filter(F.datediff(F.current_date(), F.col(\"date\")) \u003c\u003d 30)\nresult \u003d filtered_df.select(\"rule_id\", \"rule_title\", \"rule_result\").groupBy(\"rule_title\").agg(\n    F.sum(F.when(F.col(\"rule_result\") \u003d\u003d \"fail\", 1).otherwise(0)).alias(\"fail_count\")\n).orderBy(\"fail_count\", ascending\u003dFalse).limit(20)\n\nresult.write.jdbc(jdbc_url, \"top20_fail_last_month\", mode\u003d\"overwrite\", properties\u003djdbc_properties)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_avg_score_over_time \u003d df.select(\"benchmark-title\", \"date\", \"score\").withColumn(\"score\", col(\"score\").cast(\"int\")).groupBy(\"benchmark-title\", \"date\").avg(\"score\").orderBy(\"date\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_avg_score_over_time.write.jdbc(jdbc_url, \"avg_score_over_time\", mode\u003d\"overwrite\", properties\u003djdbc_properties)"
    }
  ]
}