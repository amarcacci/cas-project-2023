{
  "metadata": {
    "name": "Generate mock CIS bechmark results",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Generation of Mock CIS JSON files\nThis notebook will take 4 CIS result files stored in S3 and generate files for a given inventory for some weeks\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Read the files in the bucket `s3a://test-data` and explode the rules "
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom random import random\nfrom jinja2 import Template\nimport json\nimport datetime"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndata_json \u003d spark.read.json(\"s3a://test-data/*\", multiLine\u003dTrue)\n#data_json.show()\n\ndf_final \u003d data_json \\\n    .withColumn(\"rules_explode\", explode(\"rules\")) \\\n    .withColumn(\"rule_id\", col(\"rules_explode.rule-id\")) \\\n    .withColumn(\"rule_title\", col(\"rules_explode.rule-title\")) \\\n    .withColumn(\"rule_result\", col(\"rules_explode.result\")) \\\n    .drop(\"rules\", \"rules_explode\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Create a list of rows containing all test per OS"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparam_windows \u003d df_final.filter(col(\"benchmark-id\") \u003d\u003d \"xccdf_org.cisecurity.benchmarks_benchmark_1.0.0_CIS_Microsoft_Windows_Server_2022_Benchmark\").select(\"benchmark-id\", \"benchmark-title\", \"benchmark-version\", \"profile-id\", \"profile-title\", \"score\").limit(1).collect()[0]\nparam_ol7 \u003d df_final.filter(col(\"benchmark-id\") \u003d\u003d \"xccdf_org.cisecurity.benchmarks_benchmark_3.1.1_CIS_Oracle_Linux_7_Benchmark\").select(\"benchmark-id\", \"benchmark-title\", \"benchmark-version\", \"profile-id\", \"profile-title\", \"score\").limit(1).collect()[0]\nparam_ol8 \u003d df_final.filter(col(\"benchmark-id\") \u003d\u003d \"xccdf_org.cisecurity.benchmarks_benchmark_2.0.0_CIS_Oracle_Linux_8_Benchmark\").select(\"benchmark-id\", \"benchmark-title\", \"benchmark-version\", \"profile-id\", \"profile-title\", \"score\").limit(1).collect()[0]\nparam_rhel9 \u003d df_final.filter(col(\"benchmark-id\") \u003d\u003d \"xccdf_org.cisecurity.benchmarks_benchmark_1.0.0_CIS_Red_Hat_Enterprise_Linux_9_Benchmark\").select(\"benchmark-id\", \"benchmark-title\", \"benchmark-version\", \"profile-id\", \"profile-title\", \"score\").limit(1).collect()[0]\ncontrols_windows \u003d df_final.filter(col(\"benchmark-id\") \u003d\u003d \"xccdf_org.cisecurity.benchmarks_benchmark_1.0.0_CIS_Microsoft_Windows_Server_2022_Benchmark\").select(\"rule_id\", \"rule_title\").collect()\ncontrols_ol7 \u003d df_final.filter(col(\"benchmark-id\") \u003d\u003d \"xccdf_org.cisecurity.benchmarks_benchmark_3.1.1_CIS_Oracle_Linux_7_Benchmark\").select(\"rule_id\", \"rule_title\").collect()\ncontrols_ol8 \u003d df_final.filter(col(\"benchmark-id\") \u003d\u003d \"xccdf_org.cisecurity.benchmarks_benchmark_2.0.0_CIS_Oracle_Linux_8_Benchmark\").select(\"rule_id\", \"rule_title\").collect()\ncontrols_rhel9 \u003d df_final.filter(col(\"benchmark-id\") \u003d\u003d \"xccdf_org.cisecurity.benchmarks_benchmark_1.0.0_CIS_Red_Hat_Enterprise_Linux_9_Benchmark\").select(\"rule_id\", \"rule_title\").collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(param_windows[\u0027benchmark-id\u0027])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Inventory\nHere 4 types of inventory assets based on operating system. Each host has a quality parameter [0-1] which gives the probability that their cotnrols are pass (1 \u003d 100% probability, 0.3 \u003d 30% probability check pass)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ninventory_windows \u003d [\n    {\n        \u0027server\u0027: \u0027s0001\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0002\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0003\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0004\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0005\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0006\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0006\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0007\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0008\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0009\u0027,\n        \u0027quality\u0027: 0.7\n    }\n    \n]\ninventory_ol7 \u003d [\n    {\n        \u0027server\u0027: \u0027s0011\u0027,\n        \u0027quality\u0027: 0.5\n    },\n    {\n        \u0027server\u0027: \u0027s0012\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0013\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0014\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0015\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0016\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0017\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0018\u0027,\n        \u0027quality\u0027: 0.9\n    },\n    {\n        \u0027server\u0027: \u0027s0019\u0027,\n        \u0027quality\u0027: 0.9\n    }\n]\ninventory_ol8 \u003d [\n    {\n        \u0027server\u0027: \u0027s0021\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0022\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0023\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0024\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0025\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0026\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0027\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0028\u0027,\n        \u0027quality\u0027: 0.9\n    },\n    {\n        \u0027server\u0027: \u0027s0029\u0027,\n        \u0027quality\u0027: 0.9\n    }\n]\ninventory_rhel9 \u003d [\n    {\n        \u0027server\u0027: \u0027s0031\u0027,\n        \u0027quality\u0027: 0.4\n    },\n    {\n        \u0027server\u0027: \u0027s0032\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0033\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0034\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0035\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0036\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0037\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0038\u0027,\n        \u0027quality\u0027: 1\n    },\n    {\n        \u0027server\u0027: \u0027s0039\u0027,\n        \u0027quality\u0027: 1\n    }\n]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Definition of the quality function"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef get_result_check(quality):\n    if random() \u003c\u003d quality:\n        return \u0027pass\u0027\n    else:\n        return \u0027fail\u0027"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Definition of the CIS Benchmark template in Jinja2"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntemplate \u003d Template(\u0027\u0027\u0027\n{\n    \"server\": \"{{ server }}\",\n    \"date\": \"{{ date }}\",\n    \"benchmark-id\": \"{{ benchmark_id }}\",\n    \"benchmark-title\": \"{{ benchmark_title }}\",\n    \"benchmark-version\": \"{{ benchmark_version }}\",\n    \"profile-id\": \"{{ profile_id }}\",\n    \"profile-title\": \"{{ profile_title }}\",\n    \"score\": \"{{ score }}\",\n    \"rules\": [\n        {% for control in controls %}\n        {\n            \"rule-id\": \"{{ control.rule_id }}\",\n            \"rule-title\": \"{{ control.rule_title }}\",\n            \"result\": \"{{ control.result}}\"\n        }{% if not loop.last %},{% endif %}\n        {% endfor %}\n    ]\n}\n\u0027\u0027\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Generate windows files"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef generate_windows_files_on_date(date):\n    for item in inventory_windows:\n        server \u003d item[\u0027server\u0027]\n        date \u003d date\n        benchmark_id \u003d param_windows[\u0027benchmark-id\u0027]\n        benchmark_title \u003d param_windows[\u0027benchmark-title\u0027]\n        benchmark_version \u003d param_windows[\u0027benchmark-version\u0027]\n        profile_id \u003d param_windows[\u0027profile-id\u0027]\n        profile_title \u003d param_windows[\u0027profile-title\u0027]\n        passed \u003d 0\n        failed \u003d 0\n        graded_controls \u003d []\n        for control in controls_windows:\n            rule_title \u003d control[\u0027rule_title\u0027].replace(\u0027\\\\\u0027, \"\\\\\\\\\").replace(\"\\\"\",\"\\\\\\\"\")\n            control_rated \u003d {\n                \u0027rule_id\u0027: control[\u0027rule_id\u0027],\n                \u0027rule_title\u0027: rule_title,\n                \u0027result\u0027: get_result_check(item[\u0027quality\u0027])\n                }\n            graded_controls.append(control_rated)\n            if control_rated[\u0027result\u0027] \u003d\u003d \u0027pass\u0027:\n                passed +\u003d 1\n            if control_rated[\u0027result\u0027] \u003d\u003d \u0027fail\u0027:\n                failed +\u003d 1   \n        score \u003d (passed/(failed+passed))*100\n        \n        rendered_content \u003d template.render(\n            server\u003dserver,\n            date\u003ddate,\n            benchmark_id\u003dbenchmark_id,\n            benchmark_title \u003d benchmark_title,\n            benchmark_version \u003d benchmark_version,\n            profile_id \u003d profile_id,\n            profile_title \u003d profile_title,\n            score \u003d score,\n            controls \u003d graded_controls\n        )\n        json_content \u003d json.loads(rendered_content)\n        filename \u003d date + \"_\" + server + \".json\"\n        with open(\"/data-transfer/cis/win/\" + filename, \u0027w\u0027) as json_file:\n            json.dump(json_content, json_file, indent\u003d4)\n        print(\"(WIN) File \" + filename + \" generated!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Generate Oracle Linux 7 files"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef generate_ol7_files_on_date(date):\n    for item in inventory_ol7:\n        server \u003d item[\u0027server\u0027]\n        date \u003d date\n        benchmark_id \u003d param_ol7[\u0027benchmark-id\u0027]\n        benchmark_title \u003d param_ol7[\u0027benchmark-title\u0027]\n        benchmark_version \u003d param_ol7[\u0027benchmark-version\u0027]\n        profile_id \u003d param_ol7[\u0027profile-id\u0027]\n        profile_title \u003d param_ol7[\u0027profile-title\u0027]\n        passed \u003d 0\n        failed \u003d 0\n        graded_controls \u003d []\n        for control in controls_ol7:\n            rule_title \u003d control[\u0027rule_title\u0027].replace(\u0027\\\\\u0027, \"\\\\\\\\\").replace(\"\\\"\",\"\\\\\\\"\")\n            control_rated \u003d {\n                \u0027rule_id\u0027: control[\u0027rule_id\u0027],\n                \u0027rule_title\u0027: rule_title,\n                \u0027result\u0027: get_result_check(item[\u0027quality\u0027])\n                }\n            graded_controls.append(control_rated)\n            if control_rated[\u0027result\u0027] \u003d\u003d \u0027pass\u0027:\n                passed +\u003d 1\n            if control_rated[\u0027result\u0027] \u003d\u003d \u0027fail\u0027:\n                failed +\u003d 1   \n        score \u003d (passed/(failed+passed))*100\n        \n        rendered_content \u003d template.render(\n            server\u003dserver,\n            date\u003ddate,\n            benchmark_id\u003dbenchmark_id,\n            benchmark_title \u003d benchmark_title,\n            benchmark_version \u003d benchmark_version,\n            profile_id \u003d profile_id,\n            profile_title \u003d profile_title,\n            score \u003d score,\n            controls \u003d graded_controls\n        )\n        json_content \u003d json.loads(rendered_content)\n        filename \u003d date + \"_\" + server + \".json\"\n        with open(\"/data-transfer/cis/ol7/\" + filename, \u0027w\u0027) as json_file:\n            json.dump(json_content, json_file, indent\u003d4)\n        print(\"(OL7) File \" + filename + \" generated!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Generate Oracle Linux 8 Files"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef generate_ol8_files_on_date(date):\n    for item in inventory_ol8:\n        server \u003d item[\u0027server\u0027]\n        date \u003d date\n        benchmark_id \u003d param_ol8[\u0027benchmark-id\u0027]\n        benchmark_title \u003d param_ol8[\u0027benchmark-title\u0027]\n        benchmark_version \u003d param_ol8[\u0027benchmark-version\u0027]\n        profile_id \u003d param_ol8[\u0027profile-id\u0027]\n        profile_title \u003d param_ol8[\u0027profile-title\u0027]\n        passed \u003d 0\n        failed \u003d 0\n        graded_controls \u003d []\n        for control in controls_ol8:\n            rule_title \u003d control[\u0027rule_title\u0027].replace(\u0027\\\\\u0027, \"\\\\\\\\\").replace(\"\\\"\",\"\\\\\\\"\")\n            control_rated \u003d {\n                \u0027rule_id\u0027: control[\u0027rule_id\u0027],\n                \u0027rule_title\u0027: rule_title,\n                \u0027result\u0027: get_result_check(item[\u0027quality\u0027])\n                }\n            graded_controls.append(control_rated)\n            if control_rated[\u0027result\u0027] \u003d\u003d \u0027pass\u0027:\n                passed +\u003d 1\n            if control_rated[\u0027result\u0027] \u003d\u003d \u0027fail\u0027:\n                failed +\u003d 1   \n        score \u003d (passed/(failed+passed))*100\n        \n        rendered_content \u003d template.render(\n            server\u003dserver,\n            date\u003ddate,\n            benchmark_id\u003dbenchmark_id,\n            benchmark_title \u003d benchmark_title,\n            benchmark_version \u003d benchmark_version,\n            profile_id \u003d profile_id,\n            profile_title \u003d profile_title,\n            score \u003d score,\n            controls \u003d graded_controls\n        )\n        json_content \u003d json.loads(rendered_content)\n        filename \u003d date + \"_\" + server + \".json\"\n        with open(\"/data-transfer/cis/ol8/\" + filename, \u0027w\u0027) as json_file:\n            json.dump(json_content, json_file, indent\u003d4)\n        print(\"(OL7) File \" + filename + \" generated!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Generate Red Hat Enterprise Linux 9 Files"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef generate_rhel9_files_on_date(date):\n    for item in inventory_rhel9:\n        server \u003d item[\u0027server\u0027]\n        date \u003d date\n        benchmark_id \u003d param_rhel9[\u0027benchmark-id\u0027]\n        benchmark_title \u003d param_rhel9[\u0027benchmark-title\u0027]\n        benchmark_version \u003d param_rhel9[\u0027benchmark-version\u0027]\n        profile_id \u003d param_rhel9[\u0027profile-id\u0027]\n        profile_title \u003d param_rhel9[\u0027profile-title\u0027]\n        passed \u003d 0\n        failed \u003d 0\n        graded_controls \u003d []\n        for control in controls_rhel9:\n            rule_title \u003d control[\u0027rule_title\u0027].replace(\u0027\\\\\u0027, \"\\\\\\\\\").replace(\"\\\"\",\"\\\\\\\"\")\n            control_rated \u003d {\n                \u0027rule_id\u0027: control[\u0027rule_id\u0027],\n                \u0027rule_title\u0027: rule_title,\n                \u0027result\u0027: get_result_check(item[\u0027quality\u0027])\n                }\n            graded_controls.append(control_rated)\n            if control_rated[\u0027result\u0027] \u003d\u003d \u0027pass\u0027:\n                passed +\u003d 1\n            if control_rated[\u0027result\u0027] \u003d\u003d \u0027fail\u0027:\n                failed +\u003d 1   \n        score \u003d (passed/(failed+passed))*100\n        \n        rendered_content \u003d template.render(\n            server\u003dserver,\n            date\u003ddate,\n            benchmark_id\u003dbenchmark_id,\n            benchmark_title \u003d benchmark_title,\n            benchmark_version \u003d benchmark_version,\n            profile_id \u003d profile_id,\n            profile_title \u003d profile_title,\n            score \u003d score,\n            controls \u003d graded_controls\n        )\n        json_content \u003d json.loads(rendered_content)\n        filename \u003d date + \"_\" + server + \".json\"\n        with open(\"/data-transfer/cis/rhel9/\" + filename, \u0027w\u0027) as json_file:\n            json.dump(json_content, json_file, indent\u003d4)\n        print(\"(OL7) File \" + filename + \" generated!\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Generation of the mock files\nThis script will generate one report per week per inventory asset for the given dates"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nstart_date \u003d datetime.date(2023, 1, 1)\nend_date \u003d datetime.date(2023, 9, 1)\n\ndate_range \u003d []\n\ncurrent_date \u003d start_date\nwhile current_date \u003c\u003d end_date:\n    date_range.append(current_date)\n    current_date +\u003d datetime.timedelta(days\u003d7)\n\nfor date in date_range:\n    date_str \u003d date.strftime(\"%Y-%m-%d\")\n    print(\"Generating for date: \" + date_str)\n    generate_windows_files_on_date(date_str)\n    generate_ol7_files_on_date(date_str)\n    generate_ol8_files_on_date(date_str)\n    generate_rhel9_files_on_date(date_str)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n"
    }
  ]
}